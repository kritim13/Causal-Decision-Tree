{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Honest Inference for Population Average\n",
    "For partition $\\Pi$,the MSE is\n",
    "- averaged over observations in the test sample\n",
    "- using conditonal population means from the estimation sample\n",
    "\n",
    "Formally,\n",
    "\n",
    "$$ MSE_{\\mu}(S^{te},S^{est},\\Pi) = \\frac{i}{No. of obs. in  S^{te}}[\\underbrace{(Y_{i}-\\hat{\\mu}(X_{i}|S^{est},\\Pi))^2}_\\text{MSE Criterion} - \\overbrace{Y_{i}^2}^\\text{author addtion to simplify maths}] $$\n",
    "\n",
    "In words, this means that the tree is going to minimizes the difference between observations in the testing sample in a particular leaf and the corresponding conditional mean for that leaf calculated the estimation sample.\n",
    "\n",
    "The adjusted/expected MSE = $EMSE_{\\mu}(\\Pi)$ = $E_{S^{te},S^{est}}(MSE_{\\mu}(S^{te},S^{est},\\Pi))$\n",
    "\n",
    "#### Thus, the honest objective function/criterion is to maximize\n",
    "\n",
    "$$Q^{H}(\\Pi) = -E_{S^{te},S^{est},S^{tr}}(MSE_{\\mu}(S^{te},S^{est},\\Pi))$$\n",
    "\n",
    "###### Modification 1\n",
    "- In classical CART approach, $S^{est} = S^{tr}$\n",
    "- However, honest extimation results in anticipation of honest estimation which results in the chang in objective function :the re-estimation in the text sample is anticipated and thus, error is minimized keeping that in mind. \n",
    "- Thus, $S^{est}$ different $S^{tr}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modification 2: Honest splitting\n",
    "a. Mod 2.1: Change splitting criterion to incorporate the fact that unbiased leaf estimates will be generated using $S^{te}$ (to eliminate on source of over fitting , treating $S^{te}$  as an RV in the tree building phase.\n",
    "\n",
    "b. Mod 2.2: \"explicitly incorporate the fact that finer partitions generate greater variance in leaf estimates\" - penalize small leafs\n",
    "\n",
    "###### Developing the honest splitting criterion i.e. an analytic estimator for  $EMSE_{\\mu}(\\Pi)$\n",
    "\n",
    "Estimate expected MSE ($EMSE_{\\mu}(\\Pi)$) using only the training sample -> This will be used to place splits when training a tree.\n",
    "\n",
    "$$- EMSE_{\\mu}(\\Pi) = - E_{S^{te},S^{est}} [(Y_{i}-\\hat{\\mu}(X_{i}|S^{est},\\Pi))^2 - Y_{i}^2] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$= - E_{X_{i}}[\\mu^{2}(X_{i}|\\Pi] - E_{S^{est},X_{i}}[V(\\hat{\\mu}(X_{i}|S^{est},\\Pi)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Finding an estimator: \n",
    "given the above expansion, it is required that an estimator of $EMSE_{\\mu}(\\Pi)$ be based on only $S^{tr}$ and $N^{est}$\n",
    "\n",
    "###### a. Estimator for $V(\\hat{\\mu}(X_{i}|S^{est},\\Pi))$ \n",
    "\n",
    "As stated previously, within each leaf, there is an unbiased estimator for variance of estimted mean of the leaf.\n",
    "\n",
    "Within leaf variance = $\\hat{V}(\\hat{\\mu}(X_{i}|S^{est},\\Pi)$ = $\\frac{S_{S_{tr}}^2(l(x|\\pi)}{N^{est}(l(x|\\pi)}$\n",
    "\n",
    "To get the expected value of variance across all leafs , weigh each leaf variance by leaf share $p_{l}$ and take the sum. \n",
    "\n",
    "Formally,\n",
    "\n",
    "$E_{S^{est},X_{i}}[V(\\hat{\\mu}(X_{i}|S^{est},\\Pi)] = \\sum\\nolimits_{l}p_{l}\\frac{S_{S_{tr}}^2(l(x|\\pi)}{N^{est}(l}$\n",
    "\n",
    "Assuming that each leaf has an equal number of leafs,re-express above as\n",
    "\n",
    "$E_{S^{est},X_{i}}[V(\\hat{\\mu}(X_{i}|S^{est},\\Pi)] = \\sum\\nolimits_{l}\\frac{1}{Num leaves}\\frac{S_{S_{tr}}^2(l}{\\frac{N^{est}}{Num leaves}}$\n",
    "\n",
    "Simplifying\n",
    "\n",
    "$E_{S^{est},X_{i}}[V(\\hat{\\mu}(X_{i}|S^{est},\\Pi)] = \\frac{1}{{N^{est}}}\\sum\\nolimits_{l}S_{S_{tr}}^2(l)$\n",
    "\n",
    "###### b. Estimator for $E_{X_{i}}[\\mu^{2}(X_{i}|\\Pi)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $V(\\hat{\\mu}|x,\\Pi) = E(\\hat{\\mu}^2|x,\\Pi) - E(\\hat{\\mu}|x,\\Pi)$\n",
    "\n",
    "Applying the same logic as term 2 and using $E(\\hat{\\mu}^2(x|S,\\Pi)) = \\mu(x|\\Pi)$\n",
    "\n",
    "$$ \\frac{S_{S_{tr}}^2(l(x|\\pi))}{N^{tr}(l(x|\\pi))} = E(E(x|S^{tr},\\Pi)^2|x,\\Pi) - \\mu^2(x|\\Pi) $$\n",
    "\n",
    "Re-arranging,\n",
    "\n",
    "$$\\mu^2(x|\\Pi) = \\hat{\\mu}^2(x|S^{tr},\\Pi) - \\frac{S_{S_{tr}}^2(l(x|\\pi))}{N^{tr}(l(x|\\pi))}$$\n",
    "\n",
    "Thus, the estimator is\n",
    "\n",
    "$$\\hat{E}(\\mu^2(x|\\Pi)) = E(\\hat{\\mu}^2(x|S^{tr},\\Pi)) - E(\\frac{S_{S_{tr}}^2(l(x|\\pi))}{N^{tr}(l(x|\\pi))})$$\n",
    "\n",
    "$$\\hat{E}(\\mu^2(x|\\Pi)) = \\frac{1}{N^{tr}}\\sum\\nolimits_{i \\in S^{tr}}\\hat{\\mu}^2(x|S^{tr},\\Pi) - \\frac{1}{N^{tr}}\\sum\\nolimits_{i \\in S^{tr}}S_{S_{tr}}^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Combining both estimators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ - \\hat{EMSE_{\\mu}}(\\Pi) =  \\frac{1}{N^{tr}}\\sum\\nolimits_{i \\in S^{tr}}\\hat{\\mu}^2(x|S^{tr},\\Pi) - \\frac{1}{N^{tr}}\\sum\\nolimits_{i \\in S^{tr}}S_{S_{tr}}^2 - \\frac{1}{{N^{est}}}\\sum\\nolimits_{l}S_{S_{tr}}^2(l)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rearranging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ - \\underbrace{\\hat{EMSE_{\\mu}}(\\Pi)}_\\text{unbiased estimator of EMSE${\\mu}(\\Pi)$} =  \\underbrace{\\frac{1}{N^{tr}}\\sum\\nolimits_{i \\in S^{tr}}\\hat{\\mu}^2(x|S^{tr},\\Pi)}_\\text{a. Usual CART Criterion} - \\underbrace{(\\frac{1}{N^{tr}} +\\frac{1}{{N^{est}}})\\sum\\nolimits_{i \\in S^{tr}}S_{S_{tr}}^2(l)}_\\text{a. uncertainity about leaf mean}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimator so derived is unbiased and penalizes the creation of leaves that have high uncertainity i.e. leaves which have high variance and/or small $N^{tr},N^{est}$ as\n",
    "\n",
    "- for given x, within leaf variance is directly proportional to within leaf MSE i.e. within leaf variance will be high when within leaf MSE is high (and vice versa)\n",
    "- $(\\frac{1}{N^{tr}} +\\frac{1}{{N^{est}}})$ is inversly proportional to  $N^{tr},N^{est}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Honest Inference\n",
    "\n",
    "For each leaf the average causal effect is\n",
    "\n",
    "$$\\tau(x|\\Pi) = E[Y_{i}(1) - Y_{i}(0)|X_{i}\\in l(x|\\Pi)] =  \\mu(1,x|S,\\Pi) - \\mu(0,x|S,\\Pi)$$\n",
    "\n",
    "where\n",
    "$$\\mu(w,x|\\Pi) = \\text{the population average outcome} = \\mathop{\\mathbb{E}}[Y_{i}(W_{i})|X_{i}\\in l(x|\\Pi)]$$\n",
    "\n",
    "\n",
    "The corresponding estimator for $\\tau(x|\\Pi)$ is:\n",
    "$$\\hat{\\tau}(x|\\Pi) = \\hat{\\mu}(1,x|S,\\Pi) - \\hat{\\mu}(0,x|S,\\Pi)$$\n",
    "\n",
    "where \n",
    "$$\\hat{\\mu}(w,x|S,\\Pi) = \\underbrace{\\frac{1}{\\#(i \\in S_{w}|X_{i} \\in l(x|\\Pi))}\\sum\\nolimits_{i \\in S_{w}|X_{i} \\in l(x|\\Pi)}Y_{i}^{obs}}_\\text{average of $Y_{i}$ in leaf l when $W_{i}$ = $w_{i}$}$$\n",
    "\n",
    "Now, within a leaf, we are estimating treatment effect rather mean. Thus, the MSE is modifyied as follows:\n",
    "\n",
    "$$ MSE_{\\tau}(S^{te},S^{est},\\Pi) = \\frac{i}{No. of obs. in  S^{te}}[\\underbrace{(\\tau_{i}-\\hat{\\tau}(X_{i}|S^{est},\\Pi))^2}_\\text{MSE Criterion} - \\overbrace{\\tau_{i}^2}^\\text{author addtion to simplify maths}] $$\n",
    "\n",
    "###### However, $\\tau_{i}$ is never observed. However, $MSE_{\\tau}(S^{te},S^{est},\\Pi)$ can still be estimated by adapting $\\hat{EMSE_{\\mu}}(\\Pi)$ for $\\hat{EMSE_{\\tau}}(\\Pi)$that only depends on $S^{tr}$ and $N^{est}$\n",
    "\n",
    "$$ - \\underbrace{\\hat{EMSE_{\\tau}}(\\Pi)}_\\text{unbiased estimator of EMSE${\\tau}(\\Pi)$} =  \\underbrace{\\frac{1}{N^{tr}}\\sum\\nolimits_{i \\in S^{tr}}\\hat{\\tau}^2(x|S^{tr},\\Pi)}_\\text{a. Variance of treatment\n",
    "effects across leaves,prefers leaves with heterogenous effects} - \\underbrace{(\\frac{1}{N^{tr}} +\\frac{1}{{N^{est}}})\\sum\\nolimits_{l}(\\frac{S_{S_{treatment}}^2(l)}{p} +\\frac{S_{S_{control}}^2(l)}{1-p} )}_\\text{b. Uncertainty about leaf treatment effects:prefers leaves with good fit}$$\n",
    "\n",
    "where $$p = \\text{share of treated units} = \\frac{N_{treatment}}{N}$$\n",
    "\n",
    "Using the above as the splittin crition results in a causal tree that \"rewards a partition for finding strong heterogeneity in treatment effects (a) and penalizes a partition that creates variance in leaf estimates (b)\" (i.e. makes groups based on didiosyncratic noise).\n",
    "\n",
    "Thus, \"the tree is “causal” because it is fit by asking the data, “Where can we make a split that will produce the biggest difference in treatment effects across leaves, but still give us an accurate estimate of the treatment effect?” "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "https://scholar.princeton.edu/sites/default/files/bstewart/files/lundberg_methods_tutorial_reading_group_version.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
